{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "funzioni_progetto_WebData.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "EMOTION RECOGNITION\n",
        "\n",
        "STEP: \n",
        "-Tokenizzazione\n",
        "-Lemmatizzazione\n",
        "-Stemming\n",
        "-Rimuovere stop word\n",
        "-Trasformare tutto in minuscolo\n",
        "-far diventare 'not good' in 'not_good' (vedo dopo):/\n",
        "-Rilevamento di Bigram (token che si verificano spesso in coincidenza) utilizzando 'Phrases' di gensim  :/"
      ],
      "metadata": {
        "id": "na4lFn-4z9U_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Funzioni Topic recognition"
      ],
      "metadata": {
        "id": "R3R_5s0CeDdZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_link_menzione(doc):\n",
        "  doc=re.sub(pattern_link, '',doc)\n",
        "  doc=re.sub(pattern_menzione,\" \",doc)\n",
        "  \n",
        "  return doc"
      ],
      "metadata": {
        "id": "oL0xaRWxdKBT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clear_corpus(corpus):\n",
        "  my_not=set()\n",
        "  tmp=''\n",
        "  tokens_=[]\n",
        "  lemmatized_string=list()\n",
        "  post=list()\n",
        "\n",
        "\n",
        "  sp = spacy.load('en_core_web_sm')\n",
        "  all_stopwords = sp.Defaults.stop_words\n",
        "  wnl = WordNetLemmatizer()\n",
        "\n",
        "  for frase in corpus:\n",
        "    regex=re.compile(\"not \\w+\")\n",
        "    for a in regex.findall(frase):\n",
        "     my_not.add(a)\n",
        "  \n",
        "\n",
        "\n",
        "  tokens_ = [list(gensim.utils.tokenize(remove(doc), lower=True)) for doc in corpus]\n",
        "\n",
        "  for lista in tokens_:\n",
        "    #if len(lista)>0: #per togliermi quelle vuote che magari contenevano solo link\n",
        "    parole_esaminate=[]\n",
        "  \n",
        "    for parole in lista:\n",
        "      #if not parole in all_stopwords:\n",
        "            if parole == 'not':\n",
        "              tmp='not_'\n",
        "            elif tmp=='not_' and parole not in all_stopwords:\n",
        "                parole=tmp+parole\n",
        "                #parole_esaminate.append(parole)\n",
        "                tmp=''\n",
        "            if parole != 'not' and parole not in all_stopwords:\n",
        "              parole_esaminate.append(parole)\n",
        "\n",
        "    lemmatized_string.append([wnl.lemmatize(words) for words in parole_esaminate ]) \n",
        "\n",
        "  bigram_mdl = gensim.models.phrases.Phrases(lemmatized_string, min_count=1, threshold=2)##farglielo fare su tutto il corpus tokenizzato altrimenti non funziona se lo fai su una sola frase\n",
        "  for l in lemmatized_string:\n",
        "\n",
        "    bigrams = bigram_mdl[l]\n",
        "    post.append(bigrams)\n",
        "\n",
        "  lemmatized_bigram_string=[' '.join(word) for word in post]\n",
        "return lemmatized_bigram_string,post\n",
        "\n"
      ],
      "metadata": {
        "id": "3D-YKywdegGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tf_idfLSA(lemmatized_bigram_string):\n",
        "  modello= TfidfVectorizer()\n",
        "  M=modello.fit_transform(lemmatized_bigram_string)\n",
        "  termini=modello.get_feature_names()\n",
        "\n",
        "return modello,M,termini"
      ],
      "metadata": {
        "id": "7H1PWuahgF0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lsa(M,termini):\n",
        "  lsa = TruncatedSVD(n_components=10,n_iter=300)\n",
        "  lsa.fit(M)\n",
        "\n",
        "return lsa"
      ],
      "metadata": {
        "id": "pmOiTtQ5gUex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_concettiLSA(lsa):\n",
        "   concetti={}\n",
        "#usare questo quelli di prima sono prove\n",
        "  for i,j in enumerate(lsa.components_):\n",
        "    parolaChiave=zip(termini,abs(j))\n",
        "    paroleOrdinate = sorted(parolaChiave,key=lambda x:x[1], reverse = True)\n",
        "    paroleOrdinate = paroleOrdinate[:10]\n",
        "    concetti[\"concetto_\" + str(i)]=[]\n",
        "    print(\"Asse concettuale n. \",i, \": \")\n",
        "    for k in paroleOrdinate:\n",
        "      concetti[\"concetto_\" + str(i)].append(k)\n",
        "      print(k)\n",
        "    print('\\n\\n')\n",
        "return concetti\n"
      ],
      "metadata": {
        "id": "qtdOpO3lkITz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def punteggi_LSA(concetti,):\n",
        "  punteggi=dict()\n",
        "\n",
        "  for key,lista in concetti.items():\n",
        "    punteggio=0\n",
        "    for i in parole:\n",
        "      for concetto_parola,valore in lista:\n",
        "        if(i == concetto_parola):\n",
        "          punteggio+=valore\n",
        "    punteggi[key]=punteggio\n",
        "\n",
        "return punteggi"
      ],
      "metadata": {
        "id": "9Sx_dpxbkn_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def result_TopicLSA(corpus,punteggi):\n",
        "  risultato = []\n",
        "  print(\"Risultato di \", corpus)\n",
        "  for concetto,valore in punteggi.items():\n",
        "    if valore!=0:\n",
        "      print(concetto, concetti[concetto])"
      ],
      "metadata": {
        "id": "nzZOgWEEk47r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def result_Topic_max(corpus,punteggi):\n",
        "  risultato = []\n",
        "  massimo = max(punteggi.values())\n",
        "  print(\"Risultato di \", corpus)\n",
        "  for concetto,valore in punteggi.items():\n",
        "    if valore == massimo:\n",
        "      risultato.append((concetto, concetti[concetto]))\n",
        "  return risultato"
      ],
      "metadata": {
        "id": "RbUsT6AVlBBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lda(post):\n",
        "  parole_valore=[]\n",
        "  concetti_LDA={}\n",
        "\n",
        "  dictionary = gensim.corpora.Dictionary(post)\n",
        "  corpus2 = [dictionary.doc2bow(text) for text in post]\n",
        "\n",
        "  tfidf = gensim.models.TfidfModel(corpus2)\n",
        "  corpus_tfidf = tfidf[corpus2]\n",
        "\n",
        "  ldamodel = gensim.models.ldamodel.LdaModel(corpus2, num_topics=10, id2word=dictionary, passes=500)\n",
        "  \n",
        "\n",
        "  for idx,lista_t in topics:\n",
        "    parole_valore=[]\n",
        "    tmp_p=re.sub('[\\\"]',\"\",lista_t)\n",
        "    tmp_p=tmp_p.split(' + ') \n",
        "    coppia=list(t.split('*') for t in tmp_p)\n",
        "    concetti_LDA['concetto_'+str(idx)]=[]\n",
        "    for valore,parola in coppia:\n",
        "      concetti_LDA['concetto_'+str(idx)].append((parola,valore))\n",
        "\n",
        "return ldamodel,concetti_LDA"
      ],
      "metadata": {
        "id": "1n8RCdIWlO-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def punteggioLDA(concetti_LDA,parole):\n",
        "  punteggi=dict()\n",
        "  for key in concetti_LDA.keys():\n",
        "    punteggio=0\n",
        "    for i in parole:\n",
        "      for concetto_parola,valore in concetti_LDA[key]:\n",
        "        if(i == concetto_parola):\n",
        "         punteggio+=float(valore)\n",
        "    punteggi[key]=punteggio\n",
        "  return punteggi"
      ],
      "metadata": {
        "id": "ZHA_1rDDmifl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def result_TopicLDA(corpus,concetti_LDA):\n",
        "  risultato = []\n",
        "  p=punteggioLDA(concetti_LDA,parole)\n",
        "  massimo = max(p.values())\n",
        "  print(\"Risultato di \", corpus)\n",
        "  for concetto,valore in p.items():\n",
        "    if valore == massimo:\n",
        "      risultato.append((concetto, concetti_LDA[concetto]))\n",
        "  return (risultato)"
      ],
      "metadata": {
        "id": "W2Jb4hzNmknz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tf_idfLDA(corpusLDA):\n",
        "  tfidf = gensim.models.TfidfModel(corpusLDA)\n",
        "  corpus_tfidf = tfidf[corpusLDA]"
      ],
      "metadata": {
        "id": "g-_wl7bzlkY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def grafico_LDA(ldamodel,corpus2,dictionary):\n",
        "\n",
        "  pyLDAvis.enable_notebook()\n",
        "  lda_display = gensimvis.prepare(ldamodel, corpus2, dictionary, sort_topics=False)\n",
        "\n",
        "  pyLDAvis.display(lda_display)"
      ],
      "metadata": {
        "id": "VFyqkmARmz3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def hdp(corpus2):\n",
        "  hdpmodel = HdpModel(corpus=corpus2, id2word=dictionary)\n",
        "  hdptopics = hdpmodel.show_topics(formatted=False)"
      ],
      "metadata": {
        "id": "AT9lMTxmnH3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def punteggioHDP(concetti_HDP,parole):\n",
        "  punteggi=dict()\n",
        "  risultato = []\n",
        "  for key,lista in concetti_HDP:\n",
        "    punteggio=0\n",
        "    for i in parole:\n",
        "      for concetto_parola,valore in lista:\n",
        "        if(i == concetto_parola):\n",
        "         punteggio+=valore\n",
        "    if punteggio!=0:\n",
        "        risultato.append((key,lista))\n",
        "    punteggi[key]=punteggio\n",
        "  return punteggi"
      ],
      "metadata": {
        "id": "lTtpjQiOnWSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def result_TopicHDP(concetti_HDP,parole):\n",
        "  risultato = []\n",
        "  p=punteggioHDP(concetti_HDP,parole)\n",
        "  massimo = max(p.values())\n",
        "  #print(\"Risultato di \", corpus)\n",
        "  for concetto,valore in p.items():\n",
        "    if valore == massimo:\n",
        "      risultato.append((concetto, concetti_HDP[concetto]))\n",
        "  return (risultato)"
      ],
      "metadata": {
        "id": "DJJB_DW9ndJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "########################################################################################"
      ],
      "metadata": {
        "id": "cHcIl0yvdGhh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "  \n",
        "  #pattern_simboli=\"[!\\\"£$%&/()=?^*+\\[\\]{}#§@><\\\\|,;:_]\"  lascio # non so se togliere\n",
        "  pattern_link=\"https\\S+\"\n",
        "  #pattern_puntiCons=\"[\\.]+\"\n",
        "  pattern_menzione=\"@\\S+\"\n",
        "  #pattern_spazi=\"\\s+\"\n",
        "  #dizionarioSentiment= dict()\n",
        "   \n",
        "  sp = spacy.load('en_core_web_sm')\n",
        "  all_stopwords = sp.Defaults.stop_words\n",
        "  wnl = WordNetLemmatizer()\n",
        "\n",
        "  #content=text.lower()\n",
        "  content=re.sub(pattern_link, '',text)\n",
        "  content=re.sub(pattern_menzione,\" \",content)\n",
        "  \n",
        "  docsplit=list(gensim.utils.tokenize(content, lower=False))\n",
        "\n",
        "  return docsplit"
      ],
      "metadata": {
        "id": "lLNs4a0eenA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prova=\" assault rifle user friendly New York Medium blog assault riffle https://colab.research.google.com/drive/1vLSJBYqC8l4jtVueeNmyMhinRiVmS-tP#scrollTo=ji-puDKGhJks ciao come stai ... lol Trump FFRGGGDSS #jshdj @hsgays;  . :   not good, good  \"\n",
        "\n",
        "clean_text(prova)"
      ],
      "metadata": {
        "id": "ji-puDKGhJks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def txt_preprocessing(text):\n",
        "  \n",
        "    tmp=''\n",
        "    lemmatized_string=[]\n",
        "    parole_esaminate=[]\n",
        "    dizionario=set()\n",
        "    lst1 = list()\n",
        "    lst2 = list()\n",
        "\n",
        "    list_of_word=['impressed','sure','good','bad','happy','worry','able','fast','super','know','peaceful','cool','sad','confused','cool','attractive','gracious','sick','best','fan','care','love','sunny','funny','dead','annoying','easy','great','cold','regret','sick','like','hard','fun','enjoying','nice','bored','true','mad','angry','afraid']\n",
        "\n",
        "    bigram_mdl = gensim.models.phrases.Phrases(text, min_count=1, threshold=2)\n",
        "\n",
        "    sp = spacy.load('en_core_web_sm')\n",
        "    all_stopwords = sp.Defaults.stop_words\n",
        "    wnl = WordNetLemmatizer()\n",
        "    \n",
        "    #da fare ma meglio perchè facendolo l'accuratezza diminuiva\n",
        "    for parole in text:\n",
        "          if parole == 'not':\n",
        "            tmp='not_'\n",
        "          elif tmp=='not_' and len(parole)>2 and parole in list_of_word:\n",
        "              parole=tmp+parole\n",
        "              tmp=''\n",
        "          if parole != 'not':\n",
        "            parole_esaminate.append(parole)\n",
        "\n",
        "    lemmatized_string.append([wnl.lemmatize(words) for words in parole_esaminate if not words in all_stopwords ])\n",
        "    print(lemmatized_string)\n",
        "    bigrams = bigram_mdl[lemmatized_string]\n",
        "    pprint.pprint(list(bigrams))\n",
        "\n",
        "    #dizionario.update(lemmatized_string)\n",
        "    #lst2.append(lemmatized_string)\n",
        "\n",
        "    return bigrams"
      ],
      "metadata": {
        "id": "evi3UkY9g_J7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pprint.pprint(list(txt_preprocessing(clean_text(prova))))#non funziona bgram"
      ],
      "metadata": {
        "id": "yUiWADaOmQrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def choose_emotion_word_features(emoct1,emoct2,df):\n",
        "    df2 = df[df['sentiment'].isin([emoct1, emoct2])]\n",
        "\n",
        "#crea una lista di tuple che contengono sentiment e content\n",
        "    documenti = list(zip(df2['content'], df2['sentiment']))\n",
        "    all_words = nltk.FreqDist()\n",
        "\n",
        "\n",
        "    for coppie in documenti:\n",
        "      for parole in coppie[0]:\n",
        "         all_words[parole] += 1\n",
        "    \n",
        "    word_features = list(all_words.keys())[:len(all_words)]\n",
        "\n",
        "    return word_features\n"
      ],
      "metadata": {
        "id": "RmrjJeFTr0Ys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_features(document,word_features):\n",
        "    words = set(document)\n",
        "    features = {}\n",
        "    for w in word_features:\n",
        "        features[w] = (w in words)\n",
        "\n",
        "    return features"
      ],
      "metadata": {
        "id": "i6mocGFSu8C1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training_and_classifier(find_features,documenti):\n",
        "  featuresets = [(find_features(rev), category) for (rev, category) in documenti]\n",
        "  training_set = featuresets[:8000]#sistemare fare di più training e testing metà\n",
        "  testing_set = featuresets[5000:]\n",
        "  classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
        "\n",
        "  classifier.show_most_informative_features(10)\n",
        "  print(\"Classifier accuracy percent:\",(nltk.classify.accuracy(classifier, testing_set))*100)\n",
        "\n",
        "  return classifier"
      ],
      "metadata": {
        "id": "FYlG1THztlrl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_classifier(classifier, name):\n",
        "  save_classifier = open(name+\".pickle\",\"wb\")\n",
        "  pickle.dump(classifier, save_classifier)\n",
        "  save_classifier.close()\n"
      ],
      "metadata": {
        "id": "3q_iL1kHu32t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_classifier(name):\n",
        "  classifier_f = open(name+\".pickle\", \"rb\")\n",
        "  classifier = pickle.load(classifier_f)\n",
        "  classifier_f.close()"
      ],
      "metadata": {
        "id": "dEmdu9dqvH81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classifica(features):\n",
        "        votes = []\n",
        "        \n",
        "        categoria = classifier.classify(features)\n",
        "        votes.append(categoria)\n",
        "        return mode(votes)"
      ],
      "metadata": {
        "id": "KVqjz2tCgius"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sentiment(text):\n",
        "    feats={}\n",
        "    feature_all = find_features(text)\n",
        "    for word, value in feature_all.items():\n",
        "      if value == True:\n",
        "        feats[word]=value\n",
        "        print(word, value)\n",
        "    #print(feats)\n",
        "    return classifica(feats)\n",
        "    #,voted_classifier.confidence(feats)"
      ],
      "metadata": {
        "id": "jU8zhme5SFps"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}